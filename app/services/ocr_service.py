from __future__ import annotations

import time
import re
import io
from typing import List, Optional, Dict, Any, Tuple
from pathlib import Path

import PyPDF2
from pdf2image import convert_from_bytes
import pytesseract
from PIL import Image
import cv2
import numpy as np

from app.schemas.ocr import ExtractedText, ResumeData


class OCRService:
    """Ğ¡ĞµÑ€Ğ²Ğ¸Ñ Ğ´Ğ»Ñ OCR Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ PDF Ñ€ĞµĞ·ÑĞ¼Ğµ"""
    
    def __init__(self):
        # ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Tesseract
        self.tesseract_config = '--oem 3 --psm 6'
        
        # Ğ ĞµĞ³ÑƒĞ»ÑÑ€Ğ½Ñ‹Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
        self.patterns = {
            'name': [
                r'^([Ğ-Ğ¯Ğ][Ğ°-ÑÑ‘]+\s+[Ğ-Ğ¯Ğ][Ğ°-ÑÑ‘]+(?:\s+[Ğ-Ğ¯Ğ][Ğ°-ÑÑ‘]+)?)',
                r'Ğ¤Ğ˜Ğ[:\s]+([Ğ-Ğ¯Ğ][Ğ°-ÑÑ‘]+\s+[Ğ-Ğ¯Ğ][Ğ°-ÑÑ‘]+)',
                r'Ğ˜Ğ¼Ñ[:\s]+([Ğ-Ğ¯Ğ][Ğ°-ÑÑ‘]+)',
            ],
            'email': [
                r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
            ],
            'phone': [
                r'\+?[78][-\(]?\d{3}[-\)]?\d{3}[-]?\d{2}[-]?\d{2}',
                r'\b\d{3}[-]?\d{3}[-]?\d{2}[-]?\d{2}\b',
            ],
            'experience': [
                r'ĞĞ¿Ñ‹Ñ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹[:\s]+(.+?)(?=\n\n|\n[A-ZĞ-Ğ¯]|$)',
                r'Ğ¡Ñ‚Ğ°Ğ¶[:\s]+(.+?)(?=\n\n|\n[A-ZĞ-Ğ¯]|$)',
                r'Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»[:\s]+(.+?)(?=\n\n|\n[A-ZĞ-Ğ¯]|$)',
            ],
            'education': [
                r'ĞĞ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ[:\s]+(.+?)(?=\n\n|\n[A-ZĞ-Ğ¯]|$)',
                r'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ¸Ñ‚ĞµÑ‚[:\s]+(.+?)(?=\n\n|\n[A-ZĞ-Ğ¯]|$)',
                r'Ğ˜Ğ½ÑÑ‚Ğ¸Ñ‚ÑƒÑ‚[:\s]+(.+?)(?=\n\n|\n[A-ZĞ-Ğ¯]|$)',
            ],
            'skills': [
                r'ĞĞ°Ğ²Ñ‹ĞºĞ¸[:\s]+(.+?)(?=\n\n|\n[A-ZĞ-Ğ¯]|$)',
                r'Ğ¢ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸[:\s]+(.+?)(?=\n\n|\n[A-ZĞ-Ğ¯]|$)',
                r'Ğ—Ğ½Ğ°Ñ[:\s]+(.+?)(?=\n\n|\n[A-ZĞ-Ğ¯]|$)',
            ],
            'languages': [
                r'Ğ¯Ğ·Ñ‹ĞºĞ¸[:\s]+(.+?)(?=\n\n|\n[A-ZĞ-Ğ¯]|$)',
                r'ĞĞ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¸Ğ¹[:\s]+(.+?)(?=\n\n|\n[A-ZĞ-Ğ¯]|$)',
                r'Ğ˜Ğ½Ğ¾ÑÑ‚Ñ€Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¸[:\s]+(.+?)(?=\n\n|\n[A-ZĞ-Ğ¯]|$)',
            ],
        }
    
    async def process_pdf(self, pdf_bytes: bytes, filename: str) -> Tuple[List[ExtractedText], ResumeData]:
        """ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ PDF"""
        start_time = time.time()
        
        try:
            # Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ Ñ‚ĞµĞºÑÑ‚ Ğ¸Ğ· PDF
            extracted_text = await self._extract_text_from_pdf(pdf_bytes)
            
            # Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€ÑƒĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑĞ¼Ğµ
            resume_data = await self._extract_resume_data(extracted_text)
            
            processing_time = time.time() - start_time
            print(f"âœ… PDF Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ·Ğ° {processing_time:.2f} ÑĞµĞºÑƒĞ½Ğ´")
            
            return extracted_text, resume_data
            
        except Exception as e:
            print(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ PDF: {e}")
            raise
    
    async def _extract_text_from_pdf(self, pdf_bytes: bytes) -> List[ExtractedText]:
        """Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸Ğ· PDF Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ OCR"""
        extracted_text = []
        
        try:
            # Ğ¡Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¿Ñ€Ğ¾Ğ±ÑƒĞµĞ¼ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ÑŒ Ñ‚ĞµĞºÑÑ‚ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ¸Ğ· PDF
            pdf_text = await self._extract_pdf_text(pdf_bytes)
            
            if pdf_text and any(len(text.strip()) > 50 for text in pdf_text):
                # Ğ•ÑĞ»Ğ¸ Ñ‚ĞµĞºÑÑ‚ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ ĞµĞ³Ğ¾
                for i, text in enumerate(pdf_text):
                    if text.strip():
                        extracted_text.append(ExtractedText(
                            page_number=i + 1,
                            text=text.strip(),
                            confidence=1.0,  # PDF Ñ‚ĞµĞºÑÑ‚ Ğ¸Ğ¼ĞµĞµÑ‚ 100% ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ
                            bounding_boxes=None
                        ))
                print(f"ğŸ“„ Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½ Ñ‚ĞµĞºÑÑ‚ Ğ¸Ğ· {len(extracted_text)} ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ† PDF")
                return extracted_text
            
            # Ğ•ÑĞ»Ğ¸ PDF Ñ‚ĞµĞºÑÑ‚ Ğ½Ğµ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ OCR
            print("ğŸ” Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ OCR Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ°...")
            extracted_text = await self._ocr_extraction(pdf_bytes)
            
        except Exception as e:
            print(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ°: {e}")
            raise
        
        return extracted_text
    
    async def _extract_pdf_text(self, pdf_bytes: bytes) -> List[str]:
        """Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ¸Ğ· PDF"""
        try:
            pdf_file = io.BytesIO(pdf_bytes)
            pdf_reader = PyPDF2.PdfReader(pdf_file)
            
            texts = []
            for page in pdf_reader.pages:
                text = page.extract_text()
                texts.append(text)
            
            return texts
            
        except Exception as e:
            print(f"âš ï¸ ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ÑŒ Ñ‚ĞµĞºÑÑ‚ Ğ¸Ğ· PDF: {e}")
            return []
    
    async def _ocr_extraction(self, pdf_bytes: bytes) -> List[ExtractedText]:
        """OCR Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸Ğ· PDF"""
        extracted_text = []
        
        try:
            # ĞšĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ PDF Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ
            images = convert_from_bytes(pdf_bytes, dpi=300)
            print(f"ğŸ–¼ï¸ PDF ĞºĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ² {len(images)} Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹")
            
            for i, image in enumerate(images):
                print(f"ğŸ” ĞĞ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñƒ {i + 1}/{len(images)}...")
                
                # ĞŸÑ€ĞµĞ´Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ
                processed_image = await self._preprocess_image(image)
                
                # OCR Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ
                text, confidence, boxes = await self._extract_text_from_image(processed_image)
                
                extracted_text.append(ExtractedText(
                    page_number=i + 1,
                    text=text,
                    confidence=confidence,
                    bounding_boxes=boxes
                ))
                
                print(f"   Ğ¡Ñ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ° {i + 1}: {len(text)} ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ², ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ: {confidence:.2f}")
            
        except Exception as e:
            print(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° OCR Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ: {e}")
            raise
        
        return extracted_text
    
    async def _preprocess_image(self, image: Image.Image) -> np.ndarray:
        """ĞŸÑ€ĞµĞ´Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ OCR"""
        try:
            # ĞšĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ² numpy array
            img_array = np.array(image)
            
            # ĞšĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ² grayscale ĞµÑĞ»Ğ¸ Ğ½ÑƒĞ¶Ğ½Ğ¾
            if len(img_array.shape) == 3:
                gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)
            else:
                gray = img_array
            
            # Ğ£Ğ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚
            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
            enhanced = clahe.apply(gray)
            
            # Ğ£Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ ÑˆÑƒĞ¼
            denoised = cv2.fastNlMeansDenoising(enhanced)
            
            # Ğ‘Ğ¸Ğ½Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
            _, binary = cv2.threshold(denoised, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
            
            return binary
            
        except Exception as e:
            print(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ: {e}")
            return np.array(image)
    
    async def _extract_text_from_image(self, image: np.ndarray) -> Tuple[str, float, List[Dict[str, Any]]]:
        """Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Tesseract"""
        try:
            # OCR Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ
            ocr_data = pytesseract.image_to_data(
                image, 
                config=self.tesseract_config,
                output_type=pytesseract.Output.DICT
            )
            
            # Ğ¡Ğ¾Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ñ‚ĞµĞºÑÑ‚ Ğ¸ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ñ‹
            text_parts = []
            bounding_boxes = []
            
            for i in range(len(ocr_data['text'])):
                if int(ocr_data['conf'][i]) > 30:  # Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€ÑƒĞµĞ¼ Ğ¿Ğ¾ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸
                    text = ocr_data['text'][i].strip()
                    if text:
                        text_parts.append(text)
                        
                        # ĞšĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ñ‹ Ğ±Ğ»Ğ¾ĞºĞ°
                        box = {
                            'x': ocr_data['left'][i],
                            'y': ocr_data['top'][i],
                            'width': ocr_data['width'][i],
                            'height': ocr_data['height'][i],
                            'confidence': int(ocr_data['conf'][i]) / 100.0
                        }
                        bounding_boxes.append(box)
            
            full_text = ' '.join(text_parts)
            
            # Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµĞ¼ ÑÑ€ĞµĞ´Ğ½ÑÑ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ
            avg_confidence = np.mean([box['confidence'] for box in bounding_boxes]) if bounding_boxes else 0.0
            
            return full_text, avg_confidence, bounding_boxes
            
        except Exception as e:
            print(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° OCR Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ: {e}")
            return "", 0.0, []
    
    async def _extract_resume_data(self, extracted_text: List[ExtractedText]) -> ResumeData:
        """Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ° Ñ€ĞµĞ·ÑĞ¼Ğµ"""
        try:
            # ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµĞ¼ Ğ²ĞµÑÑŒ Ñ‚ĞµĞºÑÑ‚
            full_text = '\n'.join([et.text for et in extracted_text])
            
            # Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°Ğ¼
            extracted_data = {}
            
            for field, patterns in self.patterns.items():
                for pattern in patterns:
                    match = re.search(pattern, full_text, re.IGNORECASE | re.MULTILINE)
                    if match:
                        if field == 'name':
                            extracted_data[field] = match.group(1).strip()
                        elif field == 'email':
                            extracted_data[field] = match.group(0).strip()
                        elif field == 'phone':
                            extracted_data[field] = match.group(0).strip()
                        else:
                            extracted_data[field] = match.group(1).strip() if match.groups() else match.group(0).strip()
                        break
            
            # Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ²
            skills = self._extract_skills_from_text(full_text)
            if skills:
                extracted_data['skills'] = skills
            
            # Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° ÑĞ·Ñ‹ĞºĞ¾Ğ²
            languages = self._extract_languages_from_text(full_text)
            if languages:
                extracted_data['languages'] = languages
            
            # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚ ResumeData
            resume_data = ResumeData(
                name=extracted_data.get('name'),
                email=extracted_data.get('email'),
                phone=extracted_data.get('phone'),
                experience=extracted_data.get('experience'),
                education=extracted_data.get('education'),
                skills=extracted_data.get('skills', []),
                languages=extracted_data.get('languages', []),
                summary=self._extract_summary(full_text),
                raw_text=full_text
            )
            
            print(f"ğŸ“‹ Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ: {', '.join([k for k, v in extracted_data.items() if v])}")
            
            return resume_data
            
        except Exception as e:
            print(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑĞ¼Ğµ: {e}")
            # Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚ Ñ ÑÑ‹Ñ€Ñ‹Ğ¼ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼
            full_text = '\n'.join([et.text for et in extracted_text])
            return ResumeData(raw_text=full_text)
    
    def _extract_skills_from_text(self, text: str) -> List[str]:
        """Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°"""
        skills = []
        
        # ĞŸĞ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ²
        skill_patterns = [
            r'Python', r'Java', r'JavaScript', r'React', r'Angular', r'Vue',
            r'Node\.js', r'Django', r'Flask', r'FastAPI', r'PostgreSQL', r'MySQL',
            r'MongoDB', r'Redis', r'Docker', r'Kubernetes', r'Git', r'Linux',
            r'SQL', r'HTML', r'CSS', r'TypeScript', r'C\+\+', r'C#', r'Go',
            r'Rust', r'PHP', r'Ruby', r'Swift', r'Kotlin', r'Scala'
        ]
        
        for pattern in skill_patterns:
            if re.search(pattern, text, re.IGNORECASE):
                skills.append(pattern)
        
        return list(set(skills))  # Ğ£Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ´ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ‚Ñ‹
    
    def _extract_languages_from_text(self, text: str) -> List[str]:
        """Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°"""
        languages = []
        
        # ĞŸĞ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²
        language_patterns = [
            r'ĞĞ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¸Ğ¹[:\s]+([A-Za-z]+)', r'English[:\s]+([A-Za-z]+)',
            r'ĞĞµĞ¼ĞµÑ†ĞºĞ¸Ğ¹[:\s]+([A-Za-z]+)', r'German[:\s]+([A-Za-z]+)',
            r'Ğ¤Ñ€Ğ°Ğ½Ñ†ÑƒĞ·ÑĞºĞ¸Ğ¹[:\s]+([A-Za-z]+)', r'French[:\s]+([A-Za-z]+)',
            r'Ğ˜ÑĞ¿Ğ°Ğ½ÑĞºĞ¸Ğ¹[:\s]+([A-Za-z]+)', r'Spanish[:\s]+([A-Za-z]+)',
            r'ĞšĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ğ¹[:\s]+([A-Za-z]+)', r'Chinese[:\s]+([A-Za-z]+)',
        ]
        
        for pattern in language_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                languages.append(match.group(1))
        
        return list(set(languages))
    
    def _extract_summary(self, text: str) -> Optional[str]:
        """Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ ĞºÑ€Ğ°Ñ‚ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ"""
        # Ğ˜Ñ‰ĞµĞ¼ Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ 2-3 Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ
        sentences = re.split(r'[.!?]+', text)
        summary_sentences = [s.strip() for s in sentences[:3] if len(s.strip()) > 20]
        
        if summary_sentences:
            return '. '.join(summary_sentences) + '.'
        return None
